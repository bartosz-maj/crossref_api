{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "crossref docs: https://github.com/CrossRef/rest-api-doc#readme\n",
        "crossref py: https://github.com/fabiobatalha/crossrefapi\n",
        "crossref R: https://github.com/ropensci/rcrossref\n"
      ],
      "metadata": {
        "id": "l_cz9U1_tPDc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "APIs used list: https://www.crossref.org/  \n",
        "\n",
        "Possible: importing data into zotero?"
      ],
      "metadata": {
        "id": "37KGfeskwwbY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pip install arxiv\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-IC_FfMCvqyn",
        "outputId": "af709a16-23e2-4d45-ebbe-0a919873bf65"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting arxiv\n",
            "  Downloading arxiv-2.0.0-py3-none-any.whl (11 kB)\n",
            "Collecting feedparser==6.0.10 (from arxiv)\n",
            "  Downloading feedparser-6.0.10-py3-none-any.whl (81 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m81.1/81.1 kB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: requests==2.31.0 in /usr/local/lib/python3.10/dist-packages (from arxiv) (2.31.0)\n",
            "Collecting sgmllib3k (from feedparser==6.0.10->arxiv)\n",
            "  Downloading sgmllib3k-1.0.0.tar.gz (5.8 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests==2.31.0->arxiv) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests==2.31.0->arxiv) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests==2.31.0->arxiv) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests==2.31.0->arxiv) (2023.7.22)\n",
            "Building wheels for collected packages: sgmllib3k\n",
            "  Building wheel for sgmllib3k (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sgmllib3k: filename=sgmllib3k-1.0.0-py3-none-any.whl size=6049 sha256=6b470b4dff683741dda9c90cb63488bde78eead4e2c2592125e6b9801ba45386\n",
            "  Stored in directory: /root/.cache/pip/wheels/f0/69/93/a47e9d621be168e9e33c7ce60524393c0b92ae83cf6c6e89c5\n",
            "Successfully built sgmllib3k\n",
            "Installing collected packages: sgmllib3k, feedparser, arxiv\n",
            "Successfully installed arxiv-2.0.0 feedparser-6.0.10 sgmllib3k-1.0.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_yp4XBlRY8kT",
        "outputId": "d012df51-643a-49d6-e1d1-8bbf4e2b1548"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting crossrefapi\n",
            "  Downloading crossrefapi-1.6.0-py3-none-any.whl (14 kB)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.31.0 in /usr/local/lib/python3.10/dist-packages (from crossrefapi) (2.31.0)\n",
            "Collecting urllib3==1.26.16 (from crossrefapi)\n",
            "  Downloading urllib3-1.26.16-py2.py3-none-any.whl (143 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m143.1/143.1 kB\u001b[0m \u001b[31m2.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.31.0->crossrefapi) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.31.0->crossrefapi) (3.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.31.0->crossrefapi) (2023.7.22)\n",
            "Installing collected packages: urllib3, crossrefapi\n",
            "  Attempting uninstall: urllib3\n",
            "    Found existing installation: urllib3 2.0.7\n",
            "    Uninstalling urllib3-2.0.7:\n",
            "      Successfully uninstalled urllib3-2.0.7\n",
            "Successfully installed crossrefapi-1.6.0 urllib3-1.26.16\n"
          ]
        }
      ],
      "source": [
        "pip install crossrefapi"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from urllib.request import urlopen"
      ],
      "metadata": {
        "id": "TKmSq7iUfmiE"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from crossref.restful import Works, Etiquette"
      ],
      "metadata": {
        "id": "HLl7eHZPY9SD"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import arxiv"
      ],
      "metadata": {
        "id": "SlpN2lqMvv5J"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import json"
      ],
      "metadata": {
        "id": "npSfFfpaf1Q6"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd"
      ],
      "metadata": {
        "id": "5K6j4eLbdytx"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tqdm import tqdm"
      ],
      "metadata": {
        "id": "roMqnfoyt7SL"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Creates Etiquette entity. Adding this to the query later on should mean we are put in a \"polite\" pool of users, where the API connection should be better, and if our script is found to cause any\n",
        "# problems with the server we will be emailed to address this.\n",
        "my_etiquette = Etiquette(\"Data Access Mandate\", \"0.1\", \"https://www.adalovelaceinstitute.org/our-work/programmes/future-regulation/\", \"bmaj3035@gmail.com\")"
      ],
      "metadata": {
        "id": "KujAzta3O-HJ"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class return_articles():\n",
        "  def __init__(self, query_term, year):\n",
        "\n",
        "# The query term and year are defined here so we have access to them across all functions in an instance of this class.\n",
        "# This is useful as it means we can use them in all the functions without passing them again, saving some effort.\n",
        "    self.query_term = query_term\n",
        "    self.year = year\n",
        "\n",
        "  def generate_output_list(self):\n",
        "    print(\"--- Generating Unfiltered Output ------------------\")\n",
        "\n",
        "    # Creates an empty list.\n",
        "    output_list = []\n",
        "\n",
        "    # Creates a Works() instance, and passes the etiquette argument we created above.\n",
        "    works = Works(etiquette=my_etiquette)\n",
        "\n",
        "    # Creates a start and end date for the range we want to cover, always beginning on the 1st of January and ending on the 31st of December.\n",
        "    start_date = self.year + \"-01-01\"\n",
        "    end_date = self.year + \"-12-31\"\n",
        "\n",
        "    # Calls a query using the query_term we entered, and filters for only academic journal articles published during the year we entered.\n",
        "    # This gives us a list of entities which all represent an academic article and hold some information such as the article's title, DOI, or publisher.\n",
        "    query = works.query(self.query_term).filter(type = \"journal-article\", from_online_pub_date= start_date, until_online_pub_date = end_date)\n",
        "\n",
        "    # Prints the total number of returned items.\n",
        "    print(\"Total number of query results: \" + str(query.count()))\n",
        "\n",
        "    # Appends each item in the query variable to the empty list we created earlier.\n",
        "    for item in tqdm(query):\n",
        "      output_list.append(item)\n",
        "\n",
        "    # The function then returns the output_list when the function's code is complete.\n",
        "    return output_list\n",
        "\n",
        "  # This function requires us to pass the list of articles the previous function outputted.\n",
        "  def generate_output_titles(self, input_list):\n",
        "    print(\"--- Generating Filtered Output --------------------\")\n",
        "\n",
        "    # Creates an empty list.\n",
        "    output_json = []\n",
        "\n",
        "    # Looks through all of the entries in the input list.\n",
        "    for i in input_list:\n",
        "\n",
        "    # If the item has a \"title\" key and the query term is in the title, after it is converted to all lower case, the item is added to the empty list we created above.\n",
        "      if \"title\" in i.keys() and self.query_term in i[\"title\"][0].lower():\n",
        "        output_json.append(i)\n",
        "\n",
        "    # If the item has an \"abstract\" key, the query term is in the abstract, and the item has not already been added to the list we add it.\n",
        "      if \"abstract\" in i.keys() and self.query_term in i[\"abstract\"].lower() and i not in output_json:\n",
        "        output_json.append(i)\n",
        "\n",
        "    # Function then returns the list.\n",
        "    return output_json\n",
        "\n",
        "\n",
        "  # This function requires us to pass the list of elements outputted by the previous function and a list of the things we want our eventual CSV to hold about each article.\n",
        "  # The elements have to be written in the same way they appear in the data returned to us by the API (case sensitive).\n",
        "  def generate_output_df(self, filtered_json, required_elements):\n",
        "    print(\"--- Generating Dataframe --------------------------\")\n",
        "\n",
        "    # Creates empty list.\n",
        "    dict_list = []\n",
        "\n",
        "    # Filters through all the elements in the input list.\n",
        "    for i in range(0, len(filtered_json)):\n",
        "\n",
        "      # Creates an empty dictionary, where each item in required_elements is turned into a dictionary key. Therefore, if required_elements looked like this: [\"title\", \"DOI\", \"publisher\"]\n",
        "      # the edited_json would look like this: {\"title\": none, \"DOI\": none, \"publisher\": none}.\n",
        "      edited_json = dict.fromkeys(required_elements)\n",
        "\n",
        "      # We loop through the required elements list.\n",
        "      for j in required_elements:\n",
        "\n",
        "        # If that element is found in the article data we find it and copy it into its respective position in edited_json.\n",
        "        if j in filtered_json[i].keys():\n",
        "          edited_json[j] = filtered_json[i][j]\n",
        "\n",
        "        # If we don't find that element in our article data we paste \"no\" + required_element to the respective position in edited_json.\n",
        "        elif j not in filtered_json[i].keys():\n",
        "          edited_json[j] = \"no \" + j\n",
        "\n",
        "      # For each item in the list of articles we appen the edited_json to the empty list created at the top of this function.\n",
        "      dict_list.append(edited_json)\n",
        "\n",
        "    # Convert the list into a Pandas Data Frame to make converting into a CSV easier.\n",
        "    df = pd.DataFrame.from_records(dict_list)\n",
        "\n",
        "    # Convert the Data Frame into a CSV. This is done for each individual year. Therefore, if our connection to the server crashes we will still have CSVs\n",
        "    # for all the years we queries before the crash.\n",
        "    df.to_csv(self.query_term + \" \" + self.year + \" df.csv\", index = False)\n",
        "\n",
        "    # Returns dataframe.\n",
        "    return df\n",
        "\n",
        "  # Requires us to input a list of Data Frames\n",
        "  def output_final_df(self, df_list):\n",
        "    print(\"--- Generating CSV -------------------------------\")\n",
        "\n",
        "    # Stacks all of the input Data Frames on top of each other.\n",
        "    combined_df = pd.concat(df_list, ignore_index = True)\n",
        "\n",
        "    # Adds a https prefix to each DOI value, turning them into usable links straight away.\n",
        "    combined_df[\"DOI\"] = combined_df[\"DOI\"].apply(lambda x: \"https://doi.org/\" + x)\n",
        "\n",
        "    # Exports the combined Data Frame as a CSV.\n",
        "    combined_df.to_csv(self.query_term + \" combined.csv\", index = False)\n",
        "\n",
        "    # Returns combined Data Frame.\n",
        "    return combined_df"
      ],
      "metadata": {
        "id": "isPpUctMla9B"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# A quick function intended to create a list of years. It requires us to input a starting and ending data.\n",
        "def generate_years(min_years, max_years):\n",
        "\n",
        "  # Creates a list, where the first element is our starting year.\n",
        "  output_years = [str(min_years)]\n",
        "\n",
        "  # Creates an index value.\n",
        "  index = 1\n",
        "\n",
        "  # Creates a while loop which iterates through its code whilst the last element of the output_years loop\n",
        "  # is not equal to the max_years variable.\n",
        "  while int(output_years[-1]) != max_years:\n",
        "\n",
        "    # Adds the index value to the min_years value\n",
        "    year = min_years + index\n",
        "\n",
        "    # Appends the year value to the output_years list.\n",
        "    output_years.append(str(year))\n",
        "\n",
        "    # Adds 1 to the index, so during the next iteration of the loop the next year value is 1 greater than the\n",
        "    # year in the current iteration.\n",
        "    index += 1\n",
        "\n",
        "  # When the While loop ends because the final number is equal to the max_years variable the function returns the output_years list.\n",
        "  return output_years\n",
        "\n",
        "# Creates years list.\n",
        "years = generate_years(2010,2023)\n",
        "\n",
        "print(years)\n",
        "\n",
        "# Creates empty list.\n",
        "df_list = []"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sXttUUPaWwsH",
        "outputId": "77604a2d-8270-4690-fdb6-da188720ba0d"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['2010', '2011', '2012', '2013', '2014', '2015', '2016', '2017', '2018', '2019', '2020', '2021', '2022', '2023']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Loops through all the items in the year list.\n",
        "for i in years:\n",
        "  print(\"Generating for: \" + i)\n",
        "\n",
        "  # Creates an instance of the return_articles class, and passes the query and year arguments.\n",
        "  articles_gen = return_articles(\"access to data\", i)\n",
        "\n",
        "  # Generates articles based on the entered query and the current year.\n",
        "  unfiltered_json = articles_gen.generate_output_list()\n",
        "\n",
        "  # Filters articles ensuring only ones with the key phrase in the title or the abstract are returned.\n",
        "  filtered_json = articles_gen.generate_output_titles(unfiltered_json)\n",
        "\n",
        "  # Prints how many filtered results are outputted.\n",
        "  print(str(len(filtered_json)) + \" filtered results in the year: \" + i)\n",
        "\n",
        "  # Creates a Data Frame of the filtered articles and the required data.\n",
        "  output_df = articles_gen.generate_output_df(filtered_json, [\"title\", \"DOI\", \"publisher\",\"abstract\"])\n",
        "\n",
        "  # Appends the output_df to the empty list.\n",
        "  df_list.append(output_df)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "43i5w5SU4C2r",
        "outputId": "8fe4f021-40fa-46d0-fb11-92e56f33479d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generating for: 2010\n",
            "--- Generating Unfiltered Output ------------------\n",
            "Total number of query results: 17798\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "17798it [08:35, 34.55it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- Generating Filtered Output --------------------\n",
            "8 filtered results in the year: 2010\n",
            "--- Generating Dataframe --------------------------\n",
            "Generating for: 2011\n",
            "--- Generating Unfiltered Output ------------------\n",
            "Total number of query results: 17296\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "17296it [07:53, 36.53it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- Generating Filtered Output --------------------\n",
            "16 filtered results in the year: 2011\n",
            "--- Generating Dataframe --------------------------\n",
            "Generating for: 2012\n",
            "--- Generating Unfiltered Output ------------------\n",
            "Total number of query results: 20643\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "20643it [10:27, 32.88it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- Generating Filtered Output --------------------\n",
            "20 filtered results in the year: 2012\n",
            "--- Generating Dataframe --------------------------\n",
            "Generating for: 2013\n",
            "--- Generating Unfiltered Output ------------------\n",
            "Total number of query results: 22259\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "22259it [09:29, 39.10it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- Generating Filtered Output --------------------\n",
            "10 filtered results in the year: 2013\n",
            "--- Generating Dataframe --------------------------\n",
            "Generating for: 2014\n",
            "--- Generating Unfiltered Output ------------------\n",
            "Total number of query results: 28250\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "16201it [06:44, 46.52it/s]"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Combines all of the Data Frames into one and exports it as a CSV.\n",
        "final_df = articles_gen.output_final_df(df_list)"
      ],
      "metadata": {
        "id": "C20BybnY-kbX"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}